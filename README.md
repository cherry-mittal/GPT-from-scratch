# nanoGPT

A **minimal, fast, and readable implementation of GPT-style Transformer language models**, similar to nanoGPT created by **Andrej Karpathy**.

This is the decoder-only model, which generates text in a similar style to the input file.
---

## ğŸš€ Why nanoGPT?

* ğŸ“– **Extremely readable codebase** (single-file training loop)
* âš¡ **Fast training** using PyTorch 2.0, CUDA, and Flash Attention
* ğŸ§  **Faithful GPT architecture** (Decoder-only Transformer)
* ğŸ› ï¸ **Easy experimentation** with datasets, model sizes, and configs
* ğŸ“ **Perfect for learning LLM internals** (attention, tokens, loss, sampling)

If you want to truly *understand* how models like GPT-2 / GPT-3 work under the hood â€” this repo is gold.

---

## ğŸ§  Model Architecture

nanoGPT implements a **decoder-only Transformer** similar to GPT-2:

* Token Embeddings + Positional Embeddings
* Multi-Head Self Attention
* Feed Forward Network (MLP)
* Layer Normalization
* Residual Connections

Mathematically, it models:

> **P(xâ‚œ | xâ‚, xâ‚‚, ..., xâ‚œâ‚‹â‚)**

using causal self-attention.

---

## ğŸ“¦ Installation

### Requirements

* Python â‰¥ 3.8
* PyTorch â‰¥ 2.0
* CUDA-enabled GPU (recommended)

```bash
pip install torch numpy tqdm
```

Clone the repository:

```bash
git clone https://github.com/karpathy/nanoGPT.git
cd nanoGPT
```

---

## ğŸ“Š Dataset Preparation

### Example: Shakespeare (Character-level)

```bash
python data/shakespeare_char/prepare.py
```

This will:

* Download the dataset
* Tokenize it
* Create `train.bin` and `val.bin`

---

## ğŸ‹ï¸ Training

Train a small GPT model:

```bash
python train.py config/train_shakespeare_char.py
```

Key training features:

* Gradient accumulation
* Mixed precision (fp16 / bf16)
* Checkpointing
* Learning rate scheduling

---

## âœ¨ Text Generation

Generate text using a trained model:

```bash
python sample.py --out_dir=out-shakespeare-char
```

You can control:

* Temperature
* Top-k sampling
* Max tokens

---

## âš™ï¸ Configuration System

nanoGPT uses **Python-based configs** for full flexibility:

```python
n_layer = 6
n_head = 6
n_embd = 384
block_size = 256
batch_size = 64
learning_rate = 3e-4
```

This makes experimentation extremely fast and intuitive.

---

## ğŸ”¬ Performance

nanoGPT is optimized for speed:

* Flash Attention (when available)
* Torch compile support
* Efficient fused kernels

It can train **GPT-2 sized models** in hours instead of days on modern GPUs.

---

## ğŸ“š Learning Resources

Highly recommended companion resources:

* ğŸ¥ *Let's build GPT from scratch* â€” Andrej Karpathy (YouTube)
* ğŸ“„ *Attention Is All You Need* (Vaswani et al.)
* ğŸ“„ *GPT-2 Paper* (OpenAI)

---

## ğŸ§ª Use Cases

* Learn how LLMs work internally
* Prototype new Transformer ideas
* Train small-to-medium GPT models
* Interview preparation for ML / LLM roles
* Research & experimentation

---

## âš ï¸ Disclaimer

This project is intended for **educational and research purposes only**. It is **not** designed as a production-ready LLM system.

---

## ğŸ™Œ Credits

Inspired by:
* Andrej Karpathy's nanoGPT
* OpenAI GPT models
* PyTorch ecosystem

---

## â­ Acknowledgements

If you find this repo useful, consider giving it a â­ and supporting open-source ML education.

Happy hacking! ğŸš€
